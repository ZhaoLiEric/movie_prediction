{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 912,
     "status": "error",
     "timestamp": 1639757250133,
     "user": {
      "displayName": "Charlotte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEbTdSjYVm_WQL2obDtdI5MLbzFVZoUTPXSKeGxw=s64",
      "userId": "11842163161164919043"
     },
     "user_tz": 360
    },
    "id": "Vzf5CHtmzXMG",
    "outputId": "3c753155-e5cf-4b0a-cd75-5c2218418110"
   },
   "source": [
    "# Movie Review Prediction System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This movie review prediction system is implemented using the Graph4NLP library available at https://github.com/graph4ai/graph4nlp\n",
    "\n",
    "The movie review dataset https://www.cs.cornell.edu/people/pabo/movie-review-data/ has been pre-processed and split into train, valid and test set. It is stored in the folder `/data/MRD/raw`\n",
    "\n",
    "In order to run the training and testing code, please follow the instructions below to install necessary packages from Graph4NLP library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph4NLP library environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Install the required dependencies:\n",
    "```\n",
    "torch==1.6.0\n",
    "torchtext # >=0.7.0\n",
    "dgl==0.4.3.post2\n",
    "networkx==2.4\n",
    "nltk==3.4.5\n",
    "numpy==1.17.4\n",
    "PyYAML==5.3\n",
    "scikit-learn==0.23.1\n",
    "scipy==1.4.1\n",
    "stanfordcorenlp==3.9.1.1\n",
    "tqdm==4.47.0\n",
    "pythonds==1.2.1\n",
    "```\n",
    "\n",
    "### Create virtual environment\n",
    "```\n",
    "conda create --name graph4nlp python=3.8\n",
    "conda activate graph4nlp\n",
    "```\n",
    "\n",
    "### Install graph4nlp library\n",
    "\n",
    "\n",
    "#### Step 1: Clone the github repo of `Graph4NLP`:\n",
    "```bash\n",
    "git clone -b stable_nov2021 https://github.com/graph4ai/graph4nlp.git\n",
    "cd graph4nlp\n",
    "```\n",
    "\n",
    "#### Step 2: Do configuration\n",
    "\n",
    "Then run `./configure` (or `./configure.bat`  if you are using Windows 10) to config your installation. The configuration program will ask you to specify your CUDA version. If you do not have a GPU, please type 'cpu'.\n",
    "```bash\n",
    "./configure\n",
    "```\n",
    "\n",
    "#### Step 3: Finally, install the package:\n",
    "\n",
    "Finally, install the package:\n",
    "\n",
    "```shell\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up StanfordCoreNLP (for static graph construction)\n",
    "\n",
    "#### Step 1: Download StanfordCoreNLP\n",
    "https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "#### Step 2: Go to the root folder and start the server\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from graph4nlp.datasets.MRD import MRDDataset\n",
    "from graph4nlp.modules.graph_construction import (\n",
    "    ConstituencyBasedGraphConstruction,\n",
    "    DependencyBasedGraphConstruction,\n",
    "    IEBasedGraphConstruction,\n",
    "    NodeEmbeddingBasedGraphConstruction,\n",
    "    NodeEmbeddingBasedRefinedGraphConstruction,\n",
    ")\n",
    "from graph4nlp.modules.graph_construction.embedding_construction import WordEmbedding\n",
    "from graph4nlp.modules.graph_embedding import GAT, GGNN, GraphSAGE\n",
    "# from feedforward_nn import NNclassifier\n",
    "from graph4nlp.modules.utils import constants as Constants\n",
    "from graph4nlp.modules.utils.generic_utils import EarlyStopping, grid, to_cuda\n",
    "from graph4nlp.modules.utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8YZw5b4OzXWa"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab, label_model, cfg):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.vocab = vocab\n",
    "        self.label_model = label_model\n",
    "        embedding_style = {\n",
    "            \"single_token_item\": True if cfg[\"graph_type\"] != \"ie\" else False,\n",
    "            \"emb_strategy\": cfg.get(\"emb_strategy\", \"w2v_bilstm\"),\n",
    "            \"num_rnn_layers\": 1,\n",
    "            \"bert_model_name\": cfg.get(\"bert_model_name\", \"bert-base-uncased\"),\n",
    "            \"bert_lower_case\": True,\n",
    "        }\n",
    "\n",
    "        assert not (\n",
    "            cfg[\"graph_type\"] in (\"node_emb\", \"node_emb_refined\") and cfg[\"gnn\"] == \"gat\"\n",
    "        ), \"dynamic graph construction does not support GAT\"\n",
    "\n",
    "        use_edge_weight = False\n",
    "        if cfg[\"graph_type\"] == \"dependency\":\n",
    "            self.graph_topology = DependencyBasedGraphConstruction(\n",
    "                embedding_style=embedding_style,\n",
    "                vocab=vocab.in_word_vocab,\n",
    "                hidden_size=cfg[\"num_hidden\"],\n",
    "                word_dropout=cfg[\"word_dropout\"],\n",
    "                rnn_dropout=cfg[\"rnn_dropout\"],\n",
    "                fix_word_emb=not cfg[\"no_fix_word_emb\"],\n",
    "                fix_bert_emb=not cfg.get(\"no_fix_bert_emb\", False),\n",
    "            )\n",
    "        elif cfg[\"graph_type\"] == \"constituency\":\n",
    "            self.graph_topology = ConstituencyBasedGraphConstruction(\n",
    "                embedding_style=embedding_style,\n",
    "                vocab=vocab.in_word_vocab,\n",
    "                hidden_size=cfg[\"num_hidden\"],\n",
    "                word_dropout=cfg[\"word_dropout\"],\n",
    "                rnn_dropout=cfg[\"rnn_dropout\"],\n",
    "                fix_word_emb=not cfg[\"no_fix_word_emb\"],\n",
    "                fix_bert_emb=not cfg.get(\"no_fix_bert_emb\", False),\n",
    "            )\n",
    "        elif cfg[\"graph_type\"] == \"ie\":\n",
    "            self.graph_topology = IEBasedGraphConstruction(\n",
    "                embedding_style=embedding_style,\n",
    "                vocab=vocab.in_word_vocab,\n",
    "                hidden_size=cfg[\"num_hidden\"],\n",
    "                word_dropout=cfg[\"word_dropout\"],\n",
    "                rnn_dropout=cfg[\"rnn_dropout\"],\n",
    "                fix_word_emb=not cfg[\"no_fix_word_emb\"],\n",
    "                fix_bert_emb=not cfg.get(\"no_fix_bert_emb\", False),\n",
    "            )\n",
    "            raise RuntimeError(\"Unknown graph_type: {}\".format(cfg[\"graph_type\"]))\n",
    "\n",
    "        if \"w2v\" in self.graph_topology.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_topology.embedding_layer.word_emb_layers[\n",
    "                \"w2v\"\n",
    "            ].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(\n",
    "                self.vocab.in_word_vocab.embeddings.shape[0],\n",
    "                self.vocab.in_word_vocab.embeddings.shape[1],\n",
    "                pretrained_word_emb=self.vocab.in_word_vocab.embeddings,\n",
    "                fix_emb=not cfg[\"no_fix_word_emb\"],\n",
    "            ).word_emb_layer\n",
    "\n",
    "        if cfg[\"gnn\"] == \"gat\":\n",
    "            heads = [cfg[\"gat_num_heads\"]] * (cfg[\"gnn_num_layers\"] - 1) + [\n",
    "                cfg[\"gat_num_out_heads\"]\n",
    "            ]\n",
    "            self.gnn = GAT(\n",
    "                cfg[\"gnn_num_layers\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                heads,\n",
    "                direction_option=cfg[\"gnn_direction_option\"],\n",
    "                feat_drop=cfg[\"gnn_dropout\"],\n",
    "                attn_drop=cfg[\"gat_attn_dropout\"],\n",
    "                negative_slope=cfg[\"gat_negative_slope\"],\n",
    "                residual=cfg[\"gat_residual\"],\n",
    "                activation=F.elu,\n",
    "                allow_zero_in_degree=True,\n",
    "            )\n",
    "        elif cfg[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(\n",
    "                cfg[\"gnn_num_layers\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"graphsage_aggreagte_type\"],\n",
    "                direction_option=cfg[\"gnn_direction_option\"],\n",
    "                feat_drop=cfg[\"gnn_dropout\"],\n",
    "                bias=True,\n",
    "                norm=None,\n",
    "                activation=F.relu,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        elif cfg[\"gnn\"] == \"ggnn\":\n",
    "            self.gnn = GGNN(\n",
    "                cfg[\"gnn_num_layers\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                cfg[\"num_hidden\"],\n",
    "                feat_drop=cfg[\"gnn_dropout\"],\n",
    "                direction_option=cfg[\"gnn_direction_option\"],\n",
    "                bias=True,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown gnn type: {}\".format(cfg[\"gnn\"]))\n",
    "\n",
    "        self.clf = NNclassifier(\n",
    "            2 * cfg[\"num_hidden\"]\n",
    "            if cfg[\"gnn_direction_option\"] == \"bi_sep\"\n",
    "            else cfg[\"num_hidden\"],\n",
    "            cfg[\"num_classes\"],\n",
    "            [cfg[\"num_hidden\"]],\n",
    "            graph_pool_type=cfg[\"graph_pooling\"],\n",
    "            dim=cfg[\"num_hidden\"],\n",
    "            use_linear_proj=cfg[\"max_pool_linear_proj\"],\n",
    "        )\n",
    "\n",
    "        self.loss = nn.MSELoss(size_average=None, reduce=None, reduction=\"mean\")\n",
    "\n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        # graph embedding construction\n",
    "        batch_gd = self.graph_topology(graph_list)\n",
    "\n",
    "        # run GNN\n",
    "        self.gnn(batch_gd)\n",
    "\n",
    "        # run graph classifier\n",
    "        self.clf(batch_gd)\n",
    "        logits = batch_gd.graph_attributes[\"logits\"]\n",
    "\n",
    "        if require_loss:\n",
    "            logits = logits.to(torch.float32)\n",
    "            loss = self.loss(logits.view(-1), tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, model_path):\n",
    "        \"\"\"The API to load the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_path : str\n",
    "            The saved model path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Class\n",
    "        \"\"\"\n",
    "        return torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from torch import nn\n",
    "\n",
    "from graph4nlp.modules.utils.base import GraphClassifierBase, GraphClassifierLayerBase\n",
    "from graph4nlp.modules.utils.avg_pooling import AvgPooling\n",
    "from graph4nlp.modules.utils.max_pooling import MaxPooling\n",
    "\n",
    "class NNclassifier(GraphClassifierBase):\n",
    "    r\"\"\"NNclassifier class for graph classification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The dimension of input graph embeddings.\n",
    "    num_class : int\n",
    "        The number of classes for classification.\n",
    "    hidden_size : list of int\n",
    "        Hidden size per NN layer.\n",
    "    activation: nn.Module, optional\n",
    "        The activation function, default: `nn.ReLU()`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        num_class,\n",
    "        hidden_size,\n",
    "        activation=None,\n",
    "        graph_pool_type=\"max_pool\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(NNclassifier, self).__init__()\n",
    "\n",
    "        if not activation:\n",
    "            activation = nn.ReLU()\n",
    "\n",
    "        if graph_pool_type == \"avg_pool\":\n",
    "            self.graph_pool = AvgPooling()\n",
    "        elif graph_pool_type == \"max_pool\":\n",
    "            self.graph_pool = MaxPooling(**kwargs)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown graph pooling type: {}\".format(graph_pool_type))\n",
    "\n",
    "        self.classifier = NNclassifierLayer(input_size, num_class, hidden_size, activation)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        r\"\"\"Compute the logits tensor for graph classification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : GraphData\n",
    "            The graph data containing graph embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of GraphData\n",
    "            The output graph data containing logits tensor for graph classification.\n",
    "        \"\"\"\n",
    "        graph_emb = self.graph_pool(graph, \"node_emb\")\n",
    "        logits = self.classifier(graph_emb)\n",
    "        graph.graph_attributes[\"logits\"] = logits\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    \n",
    "class NNclassifierLayer(GraphClassifierLayerBase):\n",
    "    r\"\"\"NNclassifierLayer class for graph classification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The dimension of input graph embeddings.\n",
    "    num_class : int\n",
    "        The number of classes for classification.\n",
    "    hidden_size : list of int\n",
    "        Hidden size per NN layer.\n",
    "    activation: nn.Module, optional\n",
    "        The activation function, default: `nn.ReLU()`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_class, hidden_size, activation=None):\n",
    "        super(NNclassifierLayer, self).__init__()\n",
    "\n",
    "        if not activation:\n",
    "            activation = nn.ReLU()\n",
    "\n",
    "        # build the linear module list\n",
    "        module_seq = []\n",
    "\n",
    "        for layer_idx in range(len(hidden_size)):\n",
    "            if layer_idx == 0:\n",
    "                module_seq.append(\n",
    "                    (\"fc\" + str(layer_idx), nn.Linear(input_size, hidden_size[layer_idx]))\n",
    "                )\n",
    "            else:\n",
    "                module_seq.append(\n",
    "                    (\n",
    "                        \"fc\" + str(layer_idx),\n",
    "                        nn.Linear(hidden_size[layer_idx - 1], self.hidden_size[layer_idx]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            module_seq.append((\"activate\" + str(layer_idx), activation))\n",
    "\n",
    "        module_seq.append((\"fc_end\", nn.Linear(hidden_size[-1], num_class)))\n",
    "\n",
    "        self.classifier = nn.Sequential(collections.OrderedDict(module_seq))\n",
    "\n",
    "    def forward(self, graph_emb):\n",
    "        r\"\"\"Compute the logits tensor for graph classification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph_emb : torch.Tensor\n",
    "            The input graph embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output logits tensor for graph classification.\n",
    "        \"\"\"\n",
    "        return self.classifier(graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, cfg, logger, train_dataloader, val_dataloader, optimizer, scheduler, stopper):\n",
    "    dur = []\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        t0 = time.time()\n",
    "        for data in train_dataloader:\n",
    "            tgt = to_cuda(data[\"tgt_tensor\"], cfg[\"device\"])\n",
    "            data[\"graph_data\"] = data[\"graph_data\"].to(cfg[\"device\"])\n",
    "            logits, loss = model(data[\"graph_data\"], tgt, require_loss=True)\n",
    "\n",
    "            # add graph regularization loss if available\n",
    "            if data[\"graph_data\"].graph_attributes.get(\"graph_reg\", None) is not None:\n",
    "                loss = loss + data[\"graph_data\"].graph_attributes[\"graph_reg\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            pred = logits.cpu()\n",
    "            train_acc.append(r2_score(tgt.cpu(), pred.detach().cpu().view(-1)))\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        val_acc = evaluate(model, val_dataloader)\n",
    "        scheduler.step(val_acc)\n",
    "        print(\n",
    "            \"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} |\"\n",
    "            \"Train Acc: {:.4f} | Val Acc: {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                cfg[\"epochs\"],\n",
    "                np.mean(dur),\n",
    "                np.mean(train_loss),\n",
    "                np.mean(train_acc),\n",
    "                val_acc,\n",
    "            )\n",
    "        )\n",
    "        logger.write(\n",
    "            \"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} |\"\n",
    "            \"Train Acc: {:.4f} | Val Acc: {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                cfg[\"epochs\"],\n",
    "                np.mean(dur),\n",
    "                np.mean(train_loss),\n",
    "                np.mean(train_acc),\n",
    "                val_acc,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if stopper.step(val_acc, model):\n",
    "            break\n",
    "\n",
    "    return stopper.best_score\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        for data in dataloader:\n",
    "            tgt = to_cuda(data[\"tgt_tensor\"], cfg[\"device\"])\n",
    "            data[\"graph_data\"] = data[\"graph_data\"].to(cfg[\"device\"])\n",
    "            logits = model(data[\"graph_data\"], require_loss=False)\n",
    "            pred_collect.append(logits)\n",
    "            gt_collect.append(tgt)\n",
    "\n",
    "        pred_collect = torch.cat(pred_collect, 0).cpu()\n",
    "        gt_collect = torch.cat(gt_collect, 0).cpu()\n",
    "        score = r2_score(gt_collect, pred_collect)\n",
    "\n",
    "        return score\n",
    "\n",
    "def tester(model, cfg, logger, test_dataloader, num_test, stopper):\n",
    "    # restored best saved model\n",
    "    model = TextClassifier.load_checkpoint(stopper.save_model_path)\n",
    "\n",
    "    t0 = time.time()\n",
    "    acc = evaluate(model, test_dataloader)\n",
    "    dur = time.time() - t0\n",
    "    print(\n",
    "        \"Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}\".format(num_test, dur, acc)\n",
    "    )\n",
    "    logger.write(\n",
    "        \"Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}\".format(num_test, dur, acc)\n",
    "    )\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mQxZt3J1zhDO"
   },
   "outputs": [],
   "source": [
    "def main(cfg):\n",
    "    # configure\n",
    "    np.random.seed(cfg[\"seed\"])\n",
    "    torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    cfg[\"device\"] = torch.device(\"cuda\" if cfg[\"gpu\"] < 0 else \"cuda:%d\" % -1)\n",
    "    torch.cuda.manual_seed(cfg[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(cfg[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "    print(\"\\n\" + cfg[\"out_dir\"])\n",
    "    \n",
    "    '''\n",
    "    build logger\n",
    "    '''\n",
    "    logger = Logger(\n",
    "        cfg[\"out_dir\"],\n",
    "        config={k: v for k, v in cfg.items() if k != \"device\"},\n",
    "        overwrite=True)\n",
    "    logger.write(cfg[\"out_dir\"])\n",
    "    \n",
    "    '''\n",
    "    build dataloader\n",
    "    '''\n",
    "    dynamic_init_topology_builder = None\n",
    "    if cfg[\"graph_type\"] == \"dependency\":\n",
    "        topology_builder = DependencyBasedGraphConstruction\n",
    "        graph_type = \"static\"\n",
    "        merge_strategy = \"tailhead\"\n",
    "    elif cfg[\"graph_type\"] == \"constituency\":\n",
    "        topology_builder = ConstituencyBasedGraphConstruction\n",
    "        graph_type = \"static\"\n",
    "        merge_strategy = \"tailhead\"\n",
    "    elif cfg[\"graph_type\"] == \"ie\":\n",
    "        topology_builder = IEBasedGraphConstruction\n",
    "        graph_type = \"static\"\n",
    "        merge_strategy = \"global\"\n",
    "\n",
    "        if cfg[\"init_graph_type\"] == \"dependency\":\n",
    "            dynamic_init_topology_builder = DependencyBasedGraphConstruction\n",
    "        elif cfg[\"init_graph_type\"] == \"constituency\":\n",
    "            dynamic_init_topology_builder = ConstituencyBasedGraphConstruction\n",
    "        elif cfg[\"init_graph_type\"] == \"ie\":\n",
    "            merge_strategy = \"global\"\n",
    "            dynamic_init_topology_builder = IEBasedGraphConstruction\n",
    "        else:\n",
    "            # dynamic_init_topology_builder\n",
    "            raise RuntimeError(\"Define your own dynamic_init_topology_builder\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown graph_type: {}\".format(cfg[\"graph_type\"]))\n",
    "\n",
    "    topology_subdir = \"{}_graph\".format(cfg[\"graph_type\"])\n",
    "    if cfg[\"graph_type\"] == \"node_emb_refined\":\n",
    "        topology_subdir += \"_{}\".format(cfg[\"init_graph_type\"])\n",
    "\n",
    "    dataset = MRDDataset(\n",
    "        root_dir=cfg.get(\"root_dir\", \"data/MRD\"),\n",
    "        pretrained_word_emb_name=cfg.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "        pretrained_word_emb_cache_dir=cfg.get(\n",
    "            \"pretrained_word_emb_cache_dir\", \".vector_cache\"\n",
    "        ),\n",
    "        merge_strategy=merge_strategy,\n",
    "        seed=cfg[\"seed\"],\n",
    "        thread_number=4,\n",
    "        port=9000,\n",
    "        timeout=15000,\n",
    "        word_emb_size=300,\n",
    "        graph_type=graph_type,\n",
    "        topology_builder=topology_builder,\n",
    "        topology_subdir=topology_subdir,\n",
    "        dynamic_graph_type=cfg[\"graph_type\"]\n",
    "        if cfg[\"graph_type\"] in (\"node_emb\", \"node_emb_refined\")\n",
    "        else None,\n",
    "        dynamic_init_topology_builder=dynamic_init_topology_builder,\n",
    "        dynamic_init_topology_aux_args={\"dummy_param\": 0},\n",
    "        for_inference=False,\n",
    "        reused_vocab_model=None,\n",
    "        reused_label_model=None,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset.train,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "    if not hasattr(dataset, \"val\"):\n",
    "        dataset.val = dataset.test\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset.val,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset.test,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "    \n",
    "    vocab = dataset.vocab_model\n",
    "    label_model = dataset.label_model\n",
    "    cfg[\"num_classes\"] = 1 # label_model.num_classes\n",
    "    num_train = len(dataset.train)\n",
    "    num_val = len(dataset.val)\n",
    "    num_test = len(dataset.test)\n",
    "    print(\n",
    "        \"Train size: {}, Val size: {}, Test size: {}\".format(\n",
    "            num_train, num_val, num_test\n",
    "        )\n",
    "    )\n",
    "    logger.write(\n",
    "        \"Train size: {}, Val size: {}, Test size: {}\".format(\n",
    "            num_train, num_val, num_test\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    build model\n",
    "    '''\n",
    "    model = TextClassifier(vocab, label_model, cfg).to(cfg[\"device\"])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    build optimizer\n",
    "    '''\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(parameters, lr=cfg[\"lr\"])\n",
    "    stopper = EarlyStopping(\n",
    "        os.path.join(\n",
    "            cfg[\"out_dir\"],\n",
    "            cfg.get(\"model_ckpt_name\", Constants._SAVED_WEIGHTS_FILE)\n",
    "        ),\n",
    "        patience=cfg[\"patience\"],\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=cfg[\"lr_reduce_factor\"],\n",
    "        patience=cfg[\"lr_patience\"],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    start training and testing\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    \n",
    "    val_acc = trainer(model, cfg, logger, train_dataloader, val_dataloader, optimizer, scheduler, stopper)\n",
    "    test_acc = tester(model, cfg, logger, test_dataloader, num_test, stopper)\n",
    "\n",
    "    runtime = time.time() - t0\n",
    "    print(\"Total runtime: {:.2f}s\".format(runtime))\n",
    "    logger.write(\"Total runtime: {:.2f}s\\n\".format(runtime))\n",
    "    logger.close()\n",
    "\n",
    "    return val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SmdTd92e1A3S",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "out/MRD/ggnn_bi_fuse_dependency_ckpt\n",
      "Loading pre-built label mappings stored in data/MRD/processed/dependency_graph/label.pt\n",
      "Train size: 2975, Val size: 990, Test size: 990\n",
      "[ Fix word embeddings ]\n",
      "Epoch: [1 / 500] | Time: 8.72s | Loss: 0.0513 |Train Acc: -0.5676 | Val Acc: 0.0743\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [2 / 500] | Time: 8.56s | Loss: 0.0262 |Train Acc: 0.2130 | Val Acc: 0.2233\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [3 / 500] | Time: 8.53s | Loss: 0.0232 |Train Acc: 0.3065 | Val Acc: 0.4040\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [4 / 500] | Time: 8.51s | Loss: 0.0200 |Train Acc: 0.3956 | Val Acc: 0.4742\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [5 / 500] | Time: 8.56s | Loss: 0.0201 |Train Acc: 0.3961 | Val Acc: 0.4775\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [6 / 500] | Time: 8.57s | Loss: 0.0193 |Train Acc: 0.4238 | Val Acc: 0.5138\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [7 / 500] | Time: 8.57s | Loss: 0.0180 |Train Acc: 0.4609 | Val Acc: 0.4966\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [8 / 500] | Time: 8.57s | Loss: 0.0168 |Train Acc: 0.4905 | Val Acc: 0.4230\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: [9 / 500] | Time: 8.55s | Loss: 0.0162 |Train Acc: 0.5134 | Val Acc: 0.5493\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [10 / 500] | Time: 8.55s | Loss: 0.0164 |Train Acc: 0.5123 | Val Acc: 0.5464\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [11 / 500] | Time: 8.55s | Loss: 0.0152 |Train Acc: 0.5399 | Val Acc: 0.5615\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [12 / 500] | Time: 8.55s | Loss: 0.0142 |Train Acc: 0.5719 | Val Acc: 0.5840\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [13 / 500] | Time: 8.57s | Loss: 0.0135 |Train Acc: 0.5922 | Val Acc: 0.5532\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [14 / 500] | Time: 8.56s | Loss: 0.0132 |Train Acc: 0.6029 | Val Acc: 0.5933\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [15 / 500] | Time: 8.56s | Loss: 0.0129 |Train Acc: 0.6071 | Val Acc: 0.5401\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [16 / 500] | Time: 8.56s | Loss: 0.0123 |Train Acc: 0.6321 | Val Acc: 0.5865\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: [17 / 500] | Time: 8.56s | Loss: 0.0121 |Train Acc: 0.6256 | Val Acc: 0.5772\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [18 / 500] | Time: 8.57s | Loss: 0.0102 |Train Acc: 0.6927 | Val Acc: 0.5994\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [19 / 500] | Time: 8.57s | Loss: 0.0099 |Train Acc: 0.7026 | Val Acc: 0.6442\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [20 / 500] | Time: 8.58s | Loss: 0.0093 |Train Acc: 0.7184 | Val Acc: 0.6010\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [21 / 500] | Time: 8.58s | Loss: 0.0094 |Train Acc: 0.7145 | Val Acc: 0.6493\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [22 / 500] | Time: 8.58s | Loss: 0.0090 |Train Acc: 0.7305 | Val Acc: 0.6038\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [23 / 500] | Time: 8.59s | Loss: 0.0086 |Train Acc: 0.7352 | Val Acc: 0.6559\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [24 / 500] | Time: 8.58s | Loss: 0.0081 |Train Acc: 0.7525 | Val Acc: 0.6253\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [25 / 500] | Time: 8.59s | Loss: 0.0084 |Train Acc: 0.7456 | Val Acc: 0.5839\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: [26 / 500] | Time: 8.58s | Loss: 0.0077 |Train Acc: 0.7667 | Val Acc: 0.6573\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [27 / 500] | Time: 8.60s | Loss: 0.0075 |Train Acc: 0.7730 | Val Acc: 0.6488\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [28 / 500] | Time: 8.62s | Loss: 0.0075 |Train Acc: 0.7723 | Val Acc: 0.6586\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [29 / 500] | Time: 8.62s | Loss: 0.0069 |Train Acc: 0.7871 | Val Acc: 0.6517\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [30 / 500] | Time: 8.63s | Loss: 0.0071 |Train Acc: 0.7836 | Val Acc: 0.6140\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: [31 / 500] | Time: 8.64s | Loss: 0.0071 |Train Acc: 0.7839 | Val Acc: 0.6079\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [32 / 500] | Time: 8.65s | Loss: 0.0061 |Train Acc: 0.8147 | Val Acc: 0.6597\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [33 / 500] | Time: 8.64s | Loss: 0.0061 |Train Acc: 0.8118 | Val Acc: 0.6105\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [34 / 500] | Time: 8.66s | Loss: 0.0062 |Train Acc: 0.8135 | Val Acc: 0.6654\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [35 / 500] | Time: 8.66s | Loss: 0.0059 |Train Acc: 0.8221 | Val Acc: 0.6670\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [36 / 500] | Time: 8.66s | Loss: 0.0060 |Train Acc: 0.8148 | Val Acc: 0.6227\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [37 / 500] | Time: 8.65s | Loss: 0.0061 |Train Acc: 0.8138 | Val Acc: 0.6523\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: [38 / 500] | Time: 8.64s | Loss: 0.0056 |Train Acc: 0.8285 | Val Acc: 0.6703\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [39 / 500] | Time: 8.64s | Loss: 0.0058 |Train Acc: 0.8249 | Val Acc: 0.6267\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [40 / 500] | Time: 8.62s | Loss: 0.0055 |Train Acc: 0.8344 | Val Acc: 0.6719\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [41 / 500] | Time: 8.61s | Loss: 0.0052 |Train Acc: 0.8393 | Val Acc: 0.6565\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [42 / 500] | Time: 8.61s | Loss: 0.0052 |Train Acc: 0.8418 | Val Acc: 0.6606\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: [43 / 500] | Time: 8.60s | Loss: 0.0050 |Train Acc: 0.8466 | Val Acc: 0.6528\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [44 / 500] | Time: 8.60s | Loss: 0.0050 |Train Acc: 0.8485 | Val Acc: 0.6728\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [45 / 500] | Time: 8.61s | Loss: 0.0046 |Train Acc: 0.8578 | Val Acc: 0.6641\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [46 / 500] | Time: 8.60s | Loss: 0.0047 |Train Acc: 0.8571 | Val Acc: 0.6624\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    47: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: [47 / 500] | Time: 8.60s | Loss: 0.0047 |Train Acc: 0.8589 | Val Acc: 0.6521\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [48 / 500] | Time: 8.60s | Loss: 0.0045 |Train Acc: 0.8621 | Val Acc: 0.6590\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: [49 / 500] | Time: 8.59s | Loss: 0.0046 |Train Acc: 0.8592 | Val Acc: 0.6579\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: [50 / 500] | Time: 8.58s | Loss: 0.0045 |Train Acc: 0.8644 | Val Acc: 0.6735\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [51 / 500] | Time: 8.58s | Loss: 0.0045 |Train Acc: 0.8648 | Val Acc: 0.6743\n",
      "Saved model to out/MRD/ggnn_bi_fuse_dependency_ckpt/params.saved\n",
      "Epoch: [52 / 500] | Time: 8.57s | Loss: 0.0044 |Train Acc: 0.8649 | Val Acc: 0.6525\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [53 / 500] | Time: 8.57s | Loss: 0.0046 |Train Acc: 0.8563 | Val Acc: 0.6653\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    54: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: [54 / 500] | Time: 8.57s | Loss: 0.0043 |Train Acc: 0.8676 | Val Acc: 0.6721\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [55 / 500] | Time: 8.56s | Loss: 0.0045 |Train Acc: 0.8632 | Val Acc: 0.6713\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: [56 / 500] | Time: 8.56s | Loss: 0.0045 |Train Acc: 0.8654 | Val Acc: 0.6710\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch    57: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: [57 / 500] | Time: 8.55s | Loss: 0.0044 |Train Acc: 0.8653 | Val Acc: 0.6609\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: [58 / 500] | Time: 8.55s | Loss: 0.0044 |Train Acc: 0.8665 | Val Acc: 0.6713\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: [59 / 500] | Time: 8.55s | Loss: 0.0042 |Train Acc: 0.8705 | Val Acc: 0.6672\n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    60: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch: [60 / 500] | Time: 8.55s | Loss: 0.0044 |Train Acc: 0.8673 | Val Acc: 0.6712\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: [61 / 500] | Time: 8.54s | Loss: 0.0042 |Train Acc: 0.8717 | Val Acc: 0.6685\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Test examples: 990 | Time: 2.99s |  Test Acc: 0.6743\n",
      "Total runtime: 1226.84s\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"-config\", \"--config\", default=\"config_doc/MRD/ggnn_bi_fuse_constituency.yaml\", type=str, help=\"path to the config file\"\n",
    ")\n",
    "args = vars(parser.parse_args([]))\n",
    "with open(args[\"config\"], \"r\") as setting:\n",
    "    cfg = yaml.safe_load(setting)\n",
    "    main(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP8p0cIcKbSAtJSSD2dwIME",
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
